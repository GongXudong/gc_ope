{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC-SAC OPE Demo (FQE + DM/TIS/DR)\n",
    "\n",
    "最小示例：加载 SAC checkpoint → 采样行为数据 → 训练 FQE → 计算 DM/TIS/DR 估计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "pybullet build time: Dec 11 2025 17:43:29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/maxine/ai4robot/gc_ope')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from gc_ope.env.get_env import get_env\n",
    "from gc_ope.utils.load_config_with_hydra import load_config\n",
    "\n",
    "from gc_ope.algorithm.ope.logged_dataset import collect_logged_dataset, compute_eval_policy_cache\n",
    "from gc_ope.algorithm.ope.fqe import FQETrainer\n",
    "from gc_ope.algorithm.ope.ope_input import build_ope_inputs\n",
    "from gc_ope.algorithm.ope.estimators import (\n",
    "    dm_estimate, tis_estimate, dr_estimate,\n",
    "    dm_compute_trajectory_values, tis_compute_trajectory_values, dr_compute_trajectory_values\n",
    ")\n",
    "\n",
    "PROJECT_ROOT_DIR = Path().absolute().parent.parent.parent.parent\n",
    "PROJECT_ROOT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备环境与策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load config from: /home/maxine/ai4robot/gc_ope/configs/env_configs/flycraft/env_config_for_sac_easy.json\n",
      "3 Generator(PCG64) Generator(PCG64)\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "gamma= 0.995\n"
     ]
    }
   ],
   "source": [
    "# 准备环境与策略\n",
    "env_cfg = load_config(\n",
    "    config_path=\"../../../configs/train\",\n",
    "    config_name=\"config\",\n",
    ")\n",
    "\n",
    "# TODO: 改为从\"../../../configs/ope/config.yaml\"中读取OPE评估的环境和策略参数、行为策略采样参数、在线评估参数、FQE参数\n",
    "# TODO: 需要对应修改env_cfg.algo，env_cfg.env\n",
    "ckpt_path_1 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/sac/seed_1/best_model\"\n",
    "ckpt_path_2 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/sac/seed_2/best_model\"\n",
    "\n",
    "# 根据checkpoint路径确定环境\n",
    "if \"flycraft\" in str(ckpt_path_1):\n",
    "    env_cfg.env.env_id = \"FlyCraft-v0\"\n",
    "elif \"flycraft\" in str(ckpt_path_2):\n",
    "    env_cfg.env.env_id = \"FlyCraft-v0\"\n",
    "\n",
    "env = get_env(env_cfg.env)\n",
    "\n",
    "# 根据checkpoint路径确定策略类型\n",
    "# 注意：HER 实际上是 SAC + HerReplayBuffer，所以使用 SAC.load() 来加载\n",
    "# HER 模型需要传递 env 参数，因为 HerReplayBuffer 需要环境来初始化\n",
    "if \"sac\" in str(ckpt_path_1) or \"her\" in str(ckpt_path_1):\n",
    "    # 对于 HER 模型，需要传递 env 参数；对于普通 SAC，传递 env 也是安全的\n",
    "    behavior_algo = SAC.load(ckpt_path_1, env=env)\n",
    "    eval_algo = SAC.load(ckpt_path_2, env=env)\n",
    "elif \"ppo\" in str(ckpt_path_1):\n",
    "    behavior_algo = PPO.load(ckpt_path_1, env=env)\n",
    "    eval_algo = PPO.load(ckpt_path_2, env=env)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported algorithm: {ckpt_path_1}\")\n",
    "\n",
    "gamma = float(getattr(eval_algo, \"gamma\", 0.99))\n",
    "print(\"gamma=\", gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样行为数据，并缓存评价策略动作/对数概率\n",
    "Logged dataset from behavior policy rollouts.\n",
    "\n",
    "Contains transitions $$(s_t, a_t, r_{t+1}, s_{t+1}, \\text{done}_t)$$\n",
    "collected by rolling out a behavior policy, along with optional\n",
    "precomputed evaluation policy actions and log-probabilities.\n",
    "\n",
    "1. 没用DictReplayBuffer：除了(s,a,r,s',d)，还需要识别变长轨迹，显式存`traj_id$、`step_index`\n",
    "2. 直接在rollout时，用评价策略对s、(s,a)进行概率获取\n",
    "3. 对于状态`obs`：存两类，`dict`是原始goal-conditioned RL的状态形式，放平成`flat`\n",
    "\n",
    "Attributes:\n",
    "- obs_flat: Flattened observations (N, obs_dim).\n",
    "- actions: Actions taken by behavior policy (N, act_dim).\n",
    "- rewards: Rewards (N,).\n",
    "- next_obs_flat: Next observations (N, obs_dim).\n",
    "- dones: Episode termination flags (N,).\n",
    "- traj_id: Trajectory ID for each transition (N,).\n",
    "- step_index: Step index within trajectory (N,).\n",
    "- obs_dict: Original dict observations (list of N dicts).\n",
    "- next_obs_dict: Original dict next observations (list of N dicts)\n",
    "- behavior_log_prob: Log-probability of actions under behavior policy (N,).\n",
    "  \n",
    "*对于评价策略：（build_ope_input时计算）*\n",
    "- eval_action_curr: Evaluation policy actions at $s_t$ (N, act_dim) or None.\n",
    "- eval_action_next: Evaluation policy actions at $s_{t+1}$ (N, act_dim) or None.\n",
    "- eval_log_prob_curr: Log-probability of eval actions at $s_t$ (N,) or None.\n",
    "- eval_log_prob_next: Log-probability of eval actions at $s_{t+1}$ (N,) or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 35。target: (208.22, -8.12, -4.01)。achieved target: (212.88, -10.69, -3.40)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 186。target: (197.91, -6.81, 14.07)。achieved target: (196.67, -6.86, 11.09)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 75。target: (161.37, -2.18, 1.00)。achieved target: (164.12, 0.09, 0.82)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 81。target: (193.06, 1.74, 14.27)。achieved target: (196.12, 2.75, 11.68)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 329。target: (245.63, -4.32, 8.91)。achieved target: (236.17, -5.38, 6.14)。expert steps: 0。\n",
      "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (219.62, -4.15, -29.91)。achieved target: (204.81, -4.85, -22.17)。expert steps: 0。\n",
      "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (247.35, -4.03, -11.16)。achieved target: (210.85, -3.84, -13.43)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 93。target: (239.17, 1.70, -1.72)。achieved target: (235.67, -0.84, -2.90)。expert steps: 0。\n",
      "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (227.33, -9.39, 12.42)。achieved target: (218.46, -2.91, 5.54)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 72。target: (187.42, -8.18, 9.63)。achieved target: (186.97, -9.44, 7.64)。expert steps: 0。\n"
     ]
    }
   ],
   "source": [
    "# 采样行为数据（已实现：数据采样和评价策略缓存分离）\n",
    "# 现在 collect_logged_dataset 只进行数据采样，评价策略缓存会在 build_ope_inputs 中自动计算\n",
    "# TODO：写在python脚本里；再做日志存档\n",
    "n_episodes = 1000\n",
    "max_steps = 400\n",
    "\n",
    "dataset = collect_logged_dataset(\n",
    "    env=env,\n",
    "    behavior_algo=behavior_algo,\n",
    "    # eval_algo 参数已移除，评价策略缓存将在 build_ope_inputs 中自动计算\n",
    "    n_episodes=n_episodes,\n",
    "    max_steps=max_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'obs_dict', 'next_obs_dict', 'behavior_log_prob', 'eval_action_curr', 'eval_action_next', 'eval_log_prob_curr', 'eval_log_prob_next'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 2068 transitions from 10 episodes; obs_dim=14, act_dim=3\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Collected {len(dataset.obs_flat)} transitions from {dataset.traj_id.max() + 1} episodes; \"\n",
    "    f\"obs_dim={dataset.obs_flat.shape[1]}, act_dim={dataset.actions.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observation', 'desired_goal', 'achieved_goal'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
       "       0.5      , 0.25     ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.5, 0.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.2082162, 0.4549032, 0.4888545], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
       "       0.5      , 0.25     , 0.2082162, 0.4549032, 0.4888545, 0.2      ,\n",
       "       0.5      , 0.5      ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset.obs_dict[0].keys())\n",
    "display(dataset.obs_dict[0]['observation'])\n",
    "display(dataset.obs_dict[0]['achieved_goal'])\n",
    "display(dataset.obs_dict[0]['desired_goal'])\n",
    "display(dataset.obs_flat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPEInput准备\n",
    "**已实现：FQE 训练和预测集成到 build_ope_inputs**\n",
    "\n",
    "FQE 训练和预测过程已集成到 `build_ope_inputs` 函数中，不再需要手动训练。\n",
    "可以通过 `fqe_train_kwargs` 和 `fqe_kwargs` 参数自定义训练和初始化参数。\n",
    "\n",
    "### FQE 说明\n",
    "Fitted Q Evaluation trainer for continuous goal-conditioned policies.\n",
    "\n",
    "FQE is an off-policy evaluation method that approximates a Q function\n",
    "$Q_\\theta(s, a)$ for the evaluation policy $\\pi_\\phi(s)$.\n",
    "\n",
    "The FQE loss is:\n",
    "\n",
    "$$\n",
    "    L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_{t+1}, s_{t+1}) \\sim D}\n",
    "        \\left[ \\left( Q_\\theta(s_t, a_t) - r_{t+1}\n",
    "            - \\gamma Q_{\\theta'}(s_{t+1}, \\pi_\\phi(s_{t+1})) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "where $D$ is the logged dataset, $\\theta'$ is the target network\n",
    "parameters (soft-updated with $\\tau$), and $\\pi_\\phi(s_{t+1})$\n",
    "is the deterministic action from the evaluation policy.\n",
    "\n",
    "The trained Q function in FQE estimates evaluation metrics more accurately\n",
    "than the Q function learned during policy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONE：好像没有做goal-conditioned？现在是把整个obs('observation', 'desired_goal', 'achieved_goal')拉平作为一个obs。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造 OPE 输入\n",
    "Build OPE inputs from logged dataset and trained FQE model.\n",
    "\n",
    "Computes evaluation policy actions/log-probs and Q-values needed for\n",
    "DM, TIS, and DR estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Goal-conditioned mode: separate processing for state and goal\n",
      "obs_state_dim: 8, goal_dim: 3, obs_dim: 14\n",
      "Goal-conditioned mode: separate processing for state and goal\n",
      "obs_state_dim: 8, goal_dim: 3, obs_dim: 14\n",
      "Epoch 0001 | FQE loss=0.037\n",
      "Epoch 0050 | FQE loss=0.004\n",
      "Epoch 0100 | FQE loss=0.007\n",
      "Epoch 0150 | FQE loss=0.011\n",
      "Epoch 0200 | FQE loss=0.018\n",
      "Epoch 0250 | FQE loss=0.030\n",
      "Epoch 0300 | FQE loss=0.047\n"
     ]
    }
   ],
   "source": [
    "# 构造 OPE 输入（已实现：FQE 训练和预测集成到 build_ope_inputs）\n",
    "# FQE 训练和预测过程已集成，支持 q_function_method 参数（当前仅支持 \"fqe\"）\n",
    "# 可以通过 fqe_train_kwargs 自定义训练参数，通过 fqe_kwargs 自定义 FQE 初始化参数\n",
    "\n",
    "# 指定设备：'cuda' 或 'cpu'，如果为 None 则自动检测（优先使用 CUDA）\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 方式1：自动创建和训练 FQE（推荐）\n",
    "loss_log = []\n",
    "\n",
    "\n",
    "def _logger(epoch: int, loss: float):\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:04d} | FQE loss={loss:.3f}\")\n",
    "    loss_log.append((epoch, loss))\n",
    "\n",
    "\n",
    "inputs = build_ope_inputs(\n",
    "    dataset=dataset,\n",
    "    eval_algo=eval_algo,\n",
    "    gamma=gamma,\n",
    "    # fqe=None,  # 如果为 None，会自动创建并训练\n",
    "    # q_function_method=\"fqe\",  # 默认 \"fqe\"\n",
    "    fqe_train_kwargs={\n",
    "        \"batch_size\": 256,\n",
    "        \"n_epochs\": 300,\n",
    "        \"shuffle\": True,\n",
    "        \"logger\": _logger,\n",
    "    },\n",
    "    fqe_kwargs={\n",
    "        \"lr\": 3e-4,\n",
    "        \"tau\": 0.005,\n",
    "        \"device\": device,\n",
    "        \"obs_state_dim\": dataset.obs_dict[0]['observation'].shape[0],\n",
    "        \"goal_dim\": dataset.obs_dict[0]['desired_goal'].shape[0],\n",
    "    },\n",
    ")\n",
    "\n",
    "# 方式2：使用预训练的 FQE（向后兼容，如果需要）\n",
    "# inputs = build_ope_inputs(\n",
    "#     dataset=dataset,\n",
    "#     eval_algo=eval_algo,\n",
    "#     gamma=gamma,\n",
    "#     fqe=fqe,  # 传入已训练的 FQE\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'behavior_log_prob', 'eval_action', 'eval_log_prob', 'q_sa_behavior', 'q_sa_eval', 'gamma'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPE算法计算\n",
    "### DM\n",
    "Direct Method (DM) estimator.\n",
    "\n",
    "DM estimates the policy value using the FQE Q-function:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DM}} = \\frac{1}{N} \\sum_{i=1}^N Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
    "\n",
    "If ``initial_only=True``, only uses initial states (step_index == 0):\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DM}} = \\frac{1}{|\\mathcal{I}_0|} \\sum_{i \\in \\mathcal{I}_0} Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
    "\n",
    "where $\\mathcal{I}_0$ is the set of initial state indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIS\n",
    "Trajectory-wise Importance Sampling (TIS) estimator.\n",
    "\n",
    "TIS estimates the policy value using trajectory-level importance weights:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{TIS}} = \\frac{1}{M} \\sum_{\\tau=1}^M w_\\tau G_\\tau\n",
    "\n",
    "$$\n",
    "\n",
    "where $M$ is the number of trajectories, $G_\\tau$ is the\n",
    "discounted return of trajectory $\\tau$, and the importance weight is:\n",
    "\n",
    "$$\n",
    "\n",
    "    w_\\tau = \\prod_{t=0}^{T_\\tau-1} \\frac{\\pi_{\\text{eval}}(a_t | s_t)}{\\pi_{\\text{behavior}}(a_t | s_t)}\n",
    "        = \\exp\\left( \\sum_{t=0}^{T_\\tau-1} \\left( \\log \\pi_{\\text{eval}}(a_t | s_t)\n",
    "            - \\log \\pi_{\\text{behavior}}(a_t | s_t) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DR\n",
    "Doubly Robust (DR) estimator.\n",
    "\n",
    "DR combines importance sampling with a control variate (Q-function) to\n",
    "reduce variance. For each trajectory $\\tau$, the estimate is:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}_\\tau^{\\text{DR}} = \\sum_{t=0}^{T_\\tau-1} \\gamma^t \\left[\n",
    "        w_t (r_t - Q(s_t, a_t)) + w_{t-1} Q(s_t, \\pi_{\\text{eval}}(s_t))\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where $w_t = \\prod_{k=0}^t \\frac{\\pi_{\\text{eval}}(a_k | s_k)}{\\pi_{\\text{behavior}}(a_k | s_k)}$\n",
    "is the step-wise importance weight, $w_{-1} = 1$, and\n",
    "$Q(s_t, a_t)$ is the Q-value for the behavior action while\n",
    "$Q(s_t, \\pi_{\\text{eval}}(s_t))$ is for the evaluation policy action.\n",
    "\n",
    "The overall estimate is:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DR}} = \\frac{1}{M} \\sum_{\\tau=1}^M \\hat{V}_\\tau^{\\text{DR}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM (step-wise): EstimateResult(mean=-3.887298107147217, ci_lower=-3.897599458694458, ci_upper=-3.8750970363616943)\n",
      "TIS: EstimateResult(mean=-inf, ci_lower=nan, ci_upper=nan)\n",
      "DR: EstimateResult(mean=nan, ci_lower=nan, ci_upper=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxine/ai4robot/gc_ope/src/gc_ope/algorithm/ope/estimators.py:184: RuntimeWarning: overflow encountered in exp\n",
      "  weight = np.exp((logp_e - logp_b).sum())\n",
      "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4671: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n",
      "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: RuntimeWarning: overflow encountered in accumulate\n",
      "  return bound(*args, **kwds)\n",
      "/home/maxine/ai4robot/gc_ope/src/gc_ope/algorithm/ope/estimators.py:258: RuntimeWarning: overflow encountered in multiply\n",
      "  term = w_step * (r - q_sa) + w_prev * v_eval\n",
      "/home/maxine/ai4robot/gc_ope/src/gc_ope/algorithm/ope/estimators.py:258: RuntimeWarning: invalid value encountered in add\n",
      "  term = w_step * (r - q_sa) + w_prev * v_eval\n"
     ]
    }
   ],
   "source": [
    "# 已实现：支持多种计算 mean&ci 的方式，计算轨迹 v 值和计算 mean&ci 的过程已分离\n",
    "# 支持 ci_method 参数：\"bootstrap\"（默认）、\"normal\"、\"t_test\"\n",
    "\n",
    "# 计算估计值（支持多种 CI 方法）\n",
    "dm_all = dm_estimate(inputs, initial_only=False, ci_method=\"bootstrap\")\n",
    "# dm_init = dm_estimate(inputs, initial_only=True, ci_method=\"bootstrap\")\n",
    "tis_res = tis_estimate(inputs, ci_method=\"bootstrap\")\n",
    "dr_res = dr_estimate(inputs, ci_method=\"bootstrap\")\n",
    "\n",
    "# 也可以使用其他 CI 方法\n",
    "# dm_all_normal = dm_estimate(inputs, initial_only=False, ci_method=\"normal\")\n",
    "# tis_res_t = tis_estimate(inputs, ci_method=\"t_test\")\n",
    "\n",
    "# 如果需要单独获取轨迹级别的值（不计算 CI）\n",
    "# dm_values = dm_compute_trajectory_values(inputs, initial_only=False)\n",
    "# tis_values = tis_compute_trajectory_values(inputs)\n",
    "# dr_values = dr_compute_trajectory_values(inputs)\n",
    "\n",
    "print(\"DM (step-wise):\", dm_all)\n",
    "# print(\"DM (initial-state):\", dm_init)\n",
    "print(\"TIS:\", tis_res)\n",
    "print(\"DR:\", dr_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 101。target: (243.15, -5.86, 7.81)。achieved target: (247.15, -3.80, 9.44)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 171。target: (179.82, 4.84, 13.33)。achieved target: (175.77, 4.29, 10.50)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 223。target: (171.87, 6.60, 9.46)。achieved target: (171.50, 6.48, 6.76)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 62。target: (218.28, 6.40, -4.29)。achieved target: (226.64, 9.35, -4.18)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 66。target: (225.87, 7.57, -23.86)。achieved target: (227.81, 6.17, -21.40)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 142。target: (234.98, -2.12, -1.22)。achieved target: (226.15, -0.58, -3.27)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 147。target: (164.63, 3.97, -12.48)。achieved target: (164.82, 2.09, -14.54)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 131。target: (237.11, -4.49, 3.71)。achieved target: (229.49, -2.06, 2.29)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 23。target: (189.97, 2.26, -18.20)。achieved target: (190.82, 1.80, -15.91)。expert steps: 0。\n",
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 290。target: (168.03, 4.94, 15.13)。achieved target: (170.98, 4.97, 12.13)。expert steps: 0。\n",
      "eval return:     -30.32 ± 14.16\n"
     ]
    }
   ],
   "source": [
    "# 在线评估真实回报（可选，耗时）\n",
    "if True:\n",
    "    # mean_r_b, std_r_b = evaluate_policy(behavior_algo, env, n_eval_episodes=5, deterministic=True)\n",
    "    mean_r_e, std_r_e = evaluate_policy(eval_algo, env, n_eval_episodes=10, deterministic=True)\n",
    "    # print(f\"behavior return: {mean_r_b:.2f} ± {std_r_b:.2f}\")\n",
    "    print(f\"eval return:     {mean_r_e:.2f} ± {std_r_e:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
