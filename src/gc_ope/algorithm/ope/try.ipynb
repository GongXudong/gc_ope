{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC-SAC OPE Demo (FQE + DM/TIS/DR)\n",
    "\n",
    "最小示例：加载 SAC checkpoint → 采样行为数据 → 训练 FQE → 计算 DM/TIS/DR 估计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "pybullet build time: Dec 11 2025 17:43:29\n",
      "/home/maxine/ai4robot/gc_ope/src/gc_ope/algorithm/ope/fqe.py:237: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_{t+1}, s_{t+1}) \\sim D}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/maxine/ai4robot/gc_ope')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import SAC, PPO, HER\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from gc_ope.env.get_env import get_env\n",
    "from gc_ope.utils.load_config_with_hydra import load_config\n",
    "\n",
    "from gc_ope.algorithm.ope.logged_dataset import collect_logged_dataset, compute_eval_policy_cache\n",
    "from gc_ope.algorithm.ope.fqe import FQETrainer\n",
    "from gc_ope.algorithm.ope.ope_input import build_ope_inputs\n",
    "from gc_ope.algorithm.ope.estimators import (\n",
    "    dm_estimate, tis_estimate, dr_estimate,\n",
    "    dm_compute_trajectory_values, tis_compute_trajectory_values, dr_compute_trajectory_values\n",
    ")\n",
    "\n",
    "PROJECT_ROOT_DIR = Path().absolute().parent.parent.parent.parent\n",
    "PROJECT_ROOT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备环境与策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load config from: /home/maxine/ai4robot/gc_ope/configs/env_configs/flycraft/env_config_for_ppo_easy.json\n",
      "3 Generator(PCG64) Generator(PCG64)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/maxine/ai4robot/gc_ope/checkpoints/flycraft/easy/sac/seed_1/rl_model_100000_steps.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m ckpt_path_2 = PROJECT_ROOT_DIR / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints/flycraft/easy/sac/seed_1/rl_model_200000_steps\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msac\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ckpt_path_1):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     behavior_algo = \u001b[43mSAC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     eval_algo = SAC.load(ckpt_path_2)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mppo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ckpt_path_1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:681\u001b[39m, in \u001b[36mBaseAlgorithm.load\u001b[39m\u001b[34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m     get_system_info()\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m data, params, pytorch_variables = \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo data found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo params found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:403\u001b[39m, in \u001b[36mload_from_zip_file\u001b[39m\u001b[34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_zip_file\u001b[39m(\n\u001b[32m    377\u001b[39m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path, io.BufferedIOBase],\n\u001b[32m    378\u001b[39m     load_data: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     print_system_info: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[32m    386\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     file = \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[32m    406\u001b[39m     device = get_device(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/functools.py:912\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    910\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:291\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    285\u001b[39m         path.parent.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:272\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    270\u001b[39m             path, suffix = newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:264\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/pathlib.py:1013\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1012\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/maxine/ai4robot/gc_ope/checkpoints/flycraft/easy/sac/seed_1/rl_model_100000_steps.zip'"
     ]
    }
   ],
   "source": [
    "# 准备环境与策略\n",
    "env_cfg = load_config(\n",
    "    config_path=\"../../../configs/train\",\n",
    "    config_name=\"config\",\n",
    ")\n",
    "\n",
    "# TODO: 改为从\"../../../configs/ope/config.yaml\"中读取OPE评估的环境和策略参数\n",
    "ckpt_path_1 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/sac/seed_1/best_model\"\n",
    "ckpt_path_2 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/sac/seed_2/best_model\"\n",
    "\n",
    "# 根据checkpoint路径确定环境\n",
    "if \"flycraft\" in str(ckpt_path_1):\n",
    "    env_cfg.env.env_id = \"FlyCraft-v0\"\n",
    "elif \"flycraft\" in str(ckpt_path_2):\n",
    "    env_cfg.env.env_id = \"FlyCraft-v0\"\n",
    "\n",
    "env = get_env(env_cfg.env)\n",
    "\n",
    "# 根据checkpoint路径确定策略类型\n",
    "if \"sac\" in str(ckpt_path_1):\n",
    "    behavior_algo = SAC.load(ckpt_path_1)\n",
    "    eval_algo = SAC.load(ckpt_path_2)\n",
    "elif \"ppo\" in str(ckpt_path_1):\n",
    "    behavior_algo = PPO.load(ckpt_path_1)\n",
    "    eval_algo = PPO.load(ckpt_path_2)\n",
    "elif \"her\" in str(ckpt_path_1):\n",
    "    behavior_algo = HER.load(ckpt_path_1)\n",
    "    eval_algo = HER.load(ckpt_path_2)\n",
    "\n",
    "gamma = float(getattr(eval_algo, \"gamma\", 0.99))\n",
    "print(\"gamma=\", gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样行为数据，并缓存评价策略动作/对数概率\n",
    "Logged dataset from behavior policy rollouts.\n",
    "\n",
    "Contains transitions $$(s_t, a_t, r_{t+1}, s_{t+1}, \\text{done}_t)$$\n",
    "collected by rolling out a behavior policy, along with optional\n",
    "precomputed evaluation policy actions and log-probabilities.\n",
    "\n",
    "1. 没用DictReplayBuffer：除了(s,a,r,s',d)，还需要识别变长轨迹，显式存`traj_id$、`step_index`\n",
    "2. 直接在rollout时，用评价策略对s、(s,a)进行概率获取\n",
    "3. 对于状态`obs`：存两类，`dict`是原始goal-conditioned RL的状态形式，放平成`flat`？用于\n",
    "\n",
    "Attributes:\n",
    "- obs_flat: Flattened observations (N, obs_dim).\n",
    "- actions: Actions taken by behavior policy (N, act_dim).\n",
    "- rewards: Rewards (N,).\n",
    "- next_obs_flat: Next observations (N, obs_dim).\n",
    "- dones: Episode termination flags (N,).\n",
    "- traj_id: Trajectory ID for each transition (N,).\n",
    "- step_index: Step index within trajectory (N,).\n",
    "- obs_dict: Original dict observations (list of N dicts).\n",
    "- next_obs_dict: Original dict next observations (list of N dicts)\n",
    "- behavior_log_prob: Log-probability of actions under behavior policy (N,).\n",
    "  \n",
    "*对于评价策略：*\n",
    "- eval_action_curr: Evaluation policy actions at $s_t$ (N, act_dim) or None.\n",
    "- eval_action_next: Evaluation policy actions at $s_{t+1}$ (N, act_dim) or None.\n",
    "- eval_log_prob_curr: Log-probability of eval actions at $s_t$ (N,) or None.\n",
    "- eval_log_prob_next: Log-probability of eval actions at $s_{t+1}$ (N,) or None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "if eval_algo is not None:\n",
    "    # a_t under eval policy on s_t and s_{t+1}\n",
    "    a_curr, _ = eval_algo.predict(obs, deterministic=True)\n",
    "    a_next, _ = eval_algo.predict(next_obs, deterministic=True)\n",
    "    a_curr_t, eval_logp_curr = _compute_action_log_prob(eval_algo, obs, a_curr)\n",
    "    a_next_t, eval_logp_next = _compute_action_log_prob(eval_algo, next_obs, a_next)\n",
    "    eval_action_curr.append(a_curr_t)\n",
    "    eval_action_next.append(a_next_t)\n",
    "    eval_log_prob_curr.append(eval_logp_curr)\n",
    "    eval_log_prob_next.append(eval_logp_next)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 34。target: (208.22, -8.12, -4.01)。achieved target: (192.55, 1.12, 21.15)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 49。target: (197.91, -6.81, 14.07)。achieved target: (185.93, 1.66, 36.61)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 65。target: (161.37, -2.18, 1.00)。achieved target: (181.41, 2.02, 48.31)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 90。target: (193.06, 1.74, 14.27)。achieved target: (164.07, 13.05, 81.55)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 40。target: (245.63, -4.32, 8.91)。achieved target: (188.89, 1.93, 30.94)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 34。target: (219.62, -4.15, -29.91)。achieved target: (192.77, 2.04, 21.67)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 30。target: (247.35, -4.03, -11.16)。achieved target: (193.65, 1.54, 19.51)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 70。target: (239.17, 1.70, -1.72)。achieved target: (173.02, 8.16, 67.22)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 44。target: (227.33, -9.39, 12.42)。achieved target: (186.67, 2.73, 34.52)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 44。target: (187.42, -8.18, 9.63)。achieved target: (188.78, 1.11, 29.95)。expert steps: 0。\n"
     ]
    }
   ],
   "source": [
    "# 采样行为数据（已实现：数据采样和评价策略缓存分离）\n",
    "# 现在 collect_logged_dataset 只进行数据采样，评价策略缓存会在 build_ope_inputs 中自动计算\n",
    "n_episodes = 10\n",
    "max_steps = 400\n",
    "\n",
    "dataset = collect_logged_dataset(\n",
    "    env=env,\n",
    "    behavior_algo=behavior_algo,\n",
    "    # eval_algo 参数已移除，评价策略缓存将在 build_ope_inputs 中自动计算\n",
    "    n_episodes=n_episodes,\n",
    "    max_steps=max_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'obs_dict', 'next_obs_dict', 'behavior_log_prob', 'eval_action_curr', 'eval_action_next', 'eval_log_prob_curr', 'eval_log_prob_next'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 500 transitions from 10 episodes; obs_dim=14, act_dim=3\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Collected {len(dataset.obs_flat)} transitions from {dataset.traj_id.max() + 1} episodes; \"\n",
    "    f\"obs_dim={dataset.obs_flat.shape[1]}, act_dim={dataset.actions.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observation', 'desired_goal', 'achieved_goal'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
       "       0.5      , 0.25     ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.5, 0.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.2082162, 0.4549032, 0.4888545], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
       "       0.5      , 0.25     , 0.2082162, 0.4549032, 0.4888545, 0.2      ,\n",
       "       0.5      , 0.5      ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset.obs_dict[0].keys())\n",
    "display(dataset.obs_dict[0]['observation'])\n",
    "display(dataset.obs_dict[0]['achieved_goal'])\n",
    "display(dataset.obs_dict[0]['desired_goal'])\n",
    "display(dataset.obs_flat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPEInput准备\n",
    "**已实现：FQE 训练和预测集成到 build_ope_inputs**\n",
    "\n",
    "FQE 训练和预测过程已集成到 `build_ope_inputs` 函数中，不再需要手动训练。\n",
    "可以通过 `fqe_train_kwargs` 和 `fqe_kwargs` 参数自定义训练和初始化参数。\n",
    "\n",
    "### FQE 说明\n",
    "Fitted Q Evaluation trainer for continuous goal-conditioned policies.\n",
    "\n",
    "FQE is an off-policy evaluation method that approximates a Q function\n",
    "$Q_\\theta(s, a)$ for the evaluation policy $\\pi_\\phi(s)$.\n",
    "\n",
    "The FQE loss is:\n",
    "\n",
    "$$\n",
    "    L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_{t+1}, s_{t+1}) \\sim D}\n",
    "        \\left[ \\left( Q_\\theta(s_t, a_t) - r_{t+1}\n",
    "            - \\gamma Q_{\\theta'}(s_{t+1}, \\pi_\\phi(s_{t+1})) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "where $D$ is the logged dataset, $\\theta'$ is the target network\n",
    "parameters (soft-updated with $\\tau$), and $\\pi_\\phi(s_{t+1})$\n",
    "is the deterministic action from the evaluation policy.\n",
    "\n",
    "The trained Q function in FQE estimates evaluation metrics more accurately\n",
    "than the Q function learned during policy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONE：好像没有做goal-conditioned？现在是把整个obs('observation', 'desired_goal', 'achieved_goal')拉平作为一个obs。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造 OPE 输入\n",
    "Build OPE inputs from logged dataset and trained FQE model.\n",
    "\n",
    "Computes evaluation policy actions/log-probs and Q-values needed for\n",
    "DM, TIS, and DR estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 | FQE loss=547.655\n",
      "Epoch 0050 | FQE loss=532.672\n",
      "Epoch 0100 | FQE loss=517.824\n",
      "Epoch 0150 | FQE loss=514.295\n",
      "Epoch 0200 | FQE loss=490.684\n",
      "Epoch 0250 | FQE loss=465.566\n",
      "Epoch 0300 | FQE loss=448.648\n"
     ]
    }
   ],
   "source": [
    "# 构造 OPE 输入（已实现：FQE 训练和预测集成到 build_ope_inputs）\n",
    "# FQE 训练和预测过程已集成，支持 q_function_method 参数（当前仅支持 \"fqe\"）\n",
    "# 可以通过 fqe_train_kwargs 自定义训练参数，通过 fqe_kwargs 自定义 FQE 初始化参数\n",
    "\n",
    "# 指定设备：'cuda' 或 'cpu'，如果为 None 则自动检测（优先使用 CUDA）\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 方式1：自动创建和训练 FQE（推荐）\n",
    "loss_log = []\n",
    "\n",
    "\n",
    "def _logger(epoch: int, loss: float):\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:04d} | FQE loss={loss:.3f}\")\n",
    "    loss_log.append((epoch, loss))\n",
    "\n",
    "\n",
    "inputs = build_ope_inputs(\n",
    "    dataset=dataset,\n",
    "    eval_algo=eval_algo,\n",
    "    gamma=gamma,\n",
    "    # fqe=None,  # 如果为 None，会自动创建并训练\n",
    "    # q_function_method=\"fqe\",  # 默认 \"fqe\"\n",
    "    fqe_train_kwargs={\n",
    "        \"batch_size\": 256,\n",
    "        \"n_epochs\": 300,\n",
    "        \"shuffle\": True,\n",
    "        \"logger\": _logger,\n",
    "    },\n",
    "    fqe_kwargs={\n",
    "        \"lr\": 3e-4,\n",
    "        \"tau\": 0.005,\n",
    "        \"device\": device,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 方式2：使用预训练的 FQE（向后兼容，如果需要）\n",
    "# inputs = build_ope_inputs(\n",
    "#     dataset=dataset,\n",
    "#     eval_algo=eval_algo,\n",
    "#     gamma=gamma,\n",
    "#     fqe=fqe,  # 传入已训练的 FQE\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'behavior_log_prob', 'eval_action', 'eval_log_prob', 'q_sa_behavior', 'q_sa_eval', 'gamma'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPE算法计算\n",
    "### DM\n",
    "Direct Method (DM) estimator.\n",
    "\n",
    "DM estimates the policy value using the FQE Q-function:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DM}} = \\frac{1}{N} \\sum_{i=1}^N Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
    "\n",
    "If ``initial_only=True``, only uses initial states (step_index == 0):\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DM}} = \\frac{1}{|\\mathcal{I}_0|} \\sum_{i \\in \\mathcal{I}_0} Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
    "\n",
    "where $\\mathcal{I}_0$ is the set of initial state indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIS\n",
    "Trajectory-wise Importance Sampling (TIS) estimator.\n",
    "\n",
    "TIS estimates the policy value using trajectory-level importance weights:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{TIS}} = \\frac{1}{M} \\sum_{\\tau=1}^M w_\\tau G_\\tau\n",
    "\n",
    "$$\n",
    "\n",
    "where $M$ is the number of trajectories, $G_\\tau$ is the\n",
    "discounted return of trajectory $\\tau$, and the importance weight is:\n",
    "\n",
    "$$\n",
    "\n",
    "    w_\\tau = \\prod_{t=0}^{T_\\tau-1} \\frac{\\pi_{\\text{eval}}(a_t | s_t)}{\\pi_{\\text{behavior}}(a_t | s_t)}\n",
    "        = \\exp\\left( \\sum_{t=0}^{T_\\tau-1} \\left( \\log \\pi_{\\text{eval}}(a_t | s_t)\n",
    "            - \\log \\pi_{\\text{behavior}}(a_t | s_t) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DR\n",
    "Doubly Robust (DR) estimator.\n",
    "\n",
    "DR combines importance sampling with a control variate (Q-function) to\n",
    "reduce variance. For each trajectory $\\tau$, the estimate is:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}_\\tau^{\\text{DR}} = \\sum_{t=0}^{T_\\tau-1} \\gamma^t \\left[\n",
    "        w_t (r_t - Q(s_t, a_t)) + w_{t-1} Q(s_t, \\pi_{\\text{eval}}(s_t))\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where $w_t = \\prod_{k=0}^t \\frac{\\pi_{\\text{eval}}(a_k | s_k)}{\\pi_{\\text{behavior}}(a_k | s_k)}$\n",
    "is the step-wise importance weight, $w_{-1} = 1$, and\n",
    "$Q(s_t, a_t)$ is the Q-value for the behavior action while\n",
    "$Q(s_t, \\pi_{\\text{eval}}(s_t))$ is for the evaluation policy action.\n",
    "\n",
    "The overall estimate is:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\hat{V}^{\\text{DR}} = \\frac{1}{M} \\sum_{\\tau=1}^M \\hat{V}_\\tau^{\\text{DR}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM (step-wise): EstimateResult(mean=-16.29575538635254, ci_lower=-16.766841888427734, ci_upper=-15.852742195129395)\n",
      "TIS: EstimateResult(mean=-5.08396668940071e+21, ci_lower=-1.525190175705199e+22, ci_upper=-7.491424636832662e-36)\n",
      "DR: EstimateResult(mean=-4.043773661166667e+21, ci_lower=-1.213131591695042e+22, ci_upper=-1319305088.0)\n"
     ]
    }
   ],
   "source": [
    "# 已实现：支持多种计算 mean&ci 的方式，计算轨迹 v 值和计算 mean&ci 的过程已分离\n",
    "# 支持 ci_method 参数：\"bootstrap\"（默认）、\"normal\"、\"t_test\"\n",
    "\n",
    "# 计算估计值（支持多种 CI 方法）\n",
    "dm_all = dm_estimate(inputs, initial_only=False, ci_method=\"bootstrap\")\n",
    "# dm_init = dm_estimate(inputs, initial_only=True, ci_method=\"bootstrap\")\n",
    "tis_res = tis_estimate(inputs, ci_method=\"bootstrap\")\n",
    "dr_res = dr_estimate(inputs, ci_method=\"bootstrap\")\n",
    "\n",
    "# 也可以使用其他 CI 方法\n",
    "# dm_all_normal = dm_estimate(inputs, initial_only=False, ci_method=\"normal\")\n",
    "# tis_res_t = tis_estimate(inputs, ci_method=\"t_test\")\n",
    "\n",
    "# 如果需要单独获取轨迹级别的值（不计算 CI）\n",
    "# dm_values = dm_compute_trajectory_values(inputs, initial_only=False)\n",
    "# tis_values = tis_compute_trajectory_values(inputs)\n",
    "# dr_values = dr_compute_trajectory_values(inputs)\n",
    "\n",
    "print(\"DM (step-wise):\", dm_all)\n",
    "# print(\"DM (initial-state):\", dm_init)\n",
    "print(\"TIS:\", tis_res)\n",
    "print(\"DR:\", dr_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print, Train, \u001b[32mreach_target_termination_single_step_based_on_angle_of_velocity_vector。\u001b[0m steps: 164。target: (235.09, -6.62, 27.86)。achieved target: (242.28, -5.59, 25.22)。expert steps: 0。\n",
      "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (212.37, 2.14, 28.23)。achieved target: (206.19, -1.36, 32.34)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 58。target: (228.70, 5.80, -26.75)。achieved target: (217.66, -5.90, 5.78)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 28。target: (186.93, -8.30, -18.39)。achieved target: (189.84, 6.87, 10.95)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 124。target: (171.39, 7.17, -22.39)。achieved target: (190.49, -13.90, 27.21)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 122。target: (179.68, -0.14, 20.97)。achieved target: (186.12, -14.12, 25.64)。expert steps: 0。\n",
      "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (246.52, 4.16, -17.18)。achieved target: (265.60, -1.20, 35.98)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 54。target: (204.50, 4.12, -26.89)。achieved target: (193.27, 9.00, 14.09)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 101。target: (217.99, -2.63, 5.38)。achieved target: (208.04, -8.86, 10.96)。expert steps: 0。\n",
      "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 59。target: (216.95, 3.38, 1.38)。achieved target: (199.45, -0.20, 9.97)。expert steps: 0。\n",
      "eval return:     -163.57 ± 43.47\n"
     ]
    }
   ],
   "source": [
    "# 在线评估真实回报（可选，耗时）\n",
    "if True:\n",
    "    # mean_r_b, std_r_b = evaluate_policy(behavior_algo, env, n_eval_episodes=5, deterministic=True)\n",
    "    mean_r_e, std_r_e = evaluate_policy(eval_algo, env, n_eval_episodes=10, deterministic=True)\n",
    "    # print(f\"behavior return: {mean_r_b:.2f} ± {std_r_b:.2f}\")\n",
    "    print(f\"eval return:     {mean_r_e:.2f} ± {std_r_e:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
