{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GC-SAC OPE Demo (FQE + DM/TIS/DR)\n",
        "\n",
        "最小示例：加载 SAC checkpoint → 采样行为数据 → 训练 FQE → 计算 DM/TIS/DR 估计。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "pybullet build time: Dec 11 2025 17:43:29\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('/home/maxine/ai4robot/gc_ope')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "from gc_ope.env.get_env import get_env\n",
        "from gc_ope.utils.load_config_with_hydra import load_config\n",
        "\n",
        "from gc_ope.algorithm.ope.logged_dataset import collect_logged_dataset\n",
        "from gc_ope.algorithm.ope.fqe import FQETrainer\n",
        "from gc_ope.algorithm.ope.ope_input import build_ope_inputs\n",
        "from gc_ope.algorithm.ope.estimators import dm_estimate, tis_estimate, dr_estimate\n",
        "\n",
        "PROJECT_ROOT_DIR = Path().absolute().parent.parent.parent.parent\n",
        "PROJECT_ROOT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 准备环境与策略"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load config from: /home/maxine/ai4robot/gc_ope/configs/env_configs/flycraft/env_config_for_ppo_easy.json\n",
            "3 Generator(PCG64) Generator(PCG64)\n",
            "gamma= 0.995\n"
          ]
        }
      ],
      "source": [
        "# 准备环境与策略\n",
        "cfg = load_config(\n",
        "    config_path=\"../../../configs/train\",\n",
        "    config_name=\"config\",\n",
        ")\n",
        "cfg.env.env_id = \"FlyCraft-v0\"\n",
        "\n",
        "env = get_env(cfg.env)\n",
        "\n",
        "ckpt_path_1 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/easy/sac/seed_1/rl_model_100000_steps\"\n",
        "ckpt_path_2 = PROJECT_ROOT_DIR / \"checkpoints/flycraft/easy/sac/seed_1/rl_model_200000_steps\"\n",
        "\n",
        "behavior_algo = SAC.load(ckpt_path_1)\n",
        "eval_algo = SAC.load(ckpt_path_2)\n",
        "\n",
        "gamma = float(getattr(eval_algo, \"gamma\", 0.99))\n",
        "print(\"gamma=\", gamma)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 采样行为数据，并缓存评价策略动作/对数概率\n",
        "Logged dataset from behavior policy rollouts.\n",
        "\n",
        "Contains transitions $$(s_t, a_t, r_{t+1}, s_{t+1}, \\text{done}_t)$$\n",
        "collected by rolling out a behavior policy, along with optional\n",
        "precomputed evaluation policy actions and log-probabilities.\n",
        "\n",
        "1. 没用DictReplayBuffer：除了(s,a,r,s',d)，还需要识别变长轨迹，显式存`traj_id$、`step_index`\n",
        "2. 直接在rollout时，用评价策略对s、(s,a)进行概率获取\n",
        "3. 对于状态`obs`：存两类，`dict`是原始goal-conditioned RL的状态形式，放平成`flat`？用于\n",
        "\n",
        "Attributes:\n",
        "- obs_flat: Flattened observations (N, obs_dim).\n",
        "- actions: Actions taken by behavior policy (N, act_dim).\n",
        "- rewards: Rewards (N,).\n",
        "- next_obs_flat: Next observations (N, obs_dim).\n",
        "- dones: Episode termination flags (N,).\n",
        "- traj_id: Trajectory ID for each transition (N,).\n",
        "- step_index: Step index within trajectory (N,).\n",
        "- obs_dict: Original dict observations (list of N dicts).\n",
        "- next_obs_dict: Original dict next observations (list of N dicts)\n",
        "- behavior_log_prob: Log-probability of actions under behavior policy (N,).\n",
        "  \n",
        "*对于评价策略：*\n",
        "- eval_action_curr: Evaluation policy actions at $s_t$ (N, act_dim) or None.\n",
        "- eval_action_next: Evaluation policy actions at $s_{t+1}$ (N, act_dim) or None.\n",
        "- eval_log_prob_curr: Log-probability of eval actions at $s_t$ (N,) or None.\n",
        "- eval_log_prob_next: Log-probability of eval actions at $s_{t+1}$ (N,) or None."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "if eval_algo is not None:\n",
        "    # a_t under eval policy on s_t and s_{t+1}\n",
        "    a_curr, _ = eval_algo.predict(obs, deterministic=True)\n",
        "    a_next, _ = eval_algo.predict(next_obs, deterministic=True)\n",
        "    a_curr_t, eval_logp_curr = _compute_action_log_prob(eval_algo, obs, a_curr)\n",
        "    a_next_t, eval_logp_next = _compute_action_log_prob(eval_algo, next_obs, a_next)\n",
        "    eval_action_curr.append(a_curr_t)\n",
        "    eval_action_next.append(a_next_t)\n",
        "    eval_log_prob_curr.append(eval_logp_curr)\n",
        "    eval_log_prob_next.append(eval_logp_next)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 34。target: (208.22, -8.12, -4.01)。achieved target: (192.55, 1.12, 21.15)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 49。target: (197.91, -6.81, 14.07)。achieved target: (185.93, 1.66, 36.61)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 65。target: (161.37, -2.18, 1.00)。achieved target: (181.41, 2.02, 48.31)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 90。target: (193.06, 1.74, 14.27)。achieved target: (164.07, 13.05, 81.55)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 40。target: (245.63, -4.32, 8.91)。achieved target: (188.89, 1.93, 30.94)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 34。target: (219.62, -4.15, -29.91)。achieved target: (192.77, 2.04, 21.67)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 30。target: (247.35, -4.03, -11.16)。achieved target: (193.65, 1.54, 19.51)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 70。target: (239.17, 1.70, -1.72)。achieved target: (173.02, 8.16, 67.22)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 44。target: (227.33, -9.39, 12.42)。achieved target: (186.67, 2.73, 34.52)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 44。target: (187.42, -8.18, 9.63)。achieved target: (188.78, 1.11, 29.95)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 39。target: (243.15, -5.86, 7.81)。achieved target: (189.20, 2.08, 29.72)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 177。target: (179.82, 4.84, 13.33)。achieved target: (125.38, 71.12, 91.19)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 116。target: (171.87, 6.60, 9.46)。achieved target: (161.32, 22.94, 89.06)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 161。target: (218.28, 6.40, -4.29)。achieved target: (116.90, 70.06, 84.10)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 140。target: (225.87, 7.57, -23.86)。achieved target: (127.26, 64.26, 80.37)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 35。target: (234.98, -2.12, -1.22)。achieved target: (192.01, 1.09, 23.94)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 23。target: (164.63, 3.97, -12.48)。achieved target: (199.84, -2.40, 6.81)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 35。target: (237.11, -4.49, 3.71)。achieved target: (191.52, 1.34, 24.45)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 78。target: (189.97, 2.26, -18.20)。achieved target: (170.89, 9.78, 69.93)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 115。target: (168.03, 4.94, 15.13)。achieved target: (161.44, 20.96, 89.94)。expert steps: 0。\n"
          ]
        }
      ],
      "source": [
        "# 采样行为数据，并缓存评价策略动作/对数概率\n",
        "#TODO: 将行为数据采样&评价策略缓存，分开，可分别设置、可复用。\n",
        "n_episodes = 20\n",
        "max_steps = 400\n",
        "\n",
        "dataset = collect_logged_dataset(\n",
        "    env=env,\n",
        "    behavior_algo=behavior_algo,\n",
        "    eval_algo=eval_algo,\n",
        "    n_episodes=n_episodes,\n",
        "    max_steps=max_steps,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'obs_dict', 'next_obs_dict', 'behavior_log_prob', 'eval_action_curr', 'eval_action_next', 'eval_log_prob_curr', 'eval_log_prob_next'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__dict__.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 1419 transitions from 20 episodes; obs_dim=14, act_dim=3\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Collected {len(dataset.obs_flat)} transitions from {dataset.traj_id.max() + 1} episodes; \"\n",
        "    f\"obs_dim={dataset.obs_flat.shape[1]}, act_dim={dataset.actions.shape[1]}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 准备OPEInput\n",
        "### 训练 FQE\n",
        "Fitted Q Evaluation trainer for continuous goal-conditioned policies.\n",
        "\n",
        "FQE is an off-policy evaluation method that approximates a Q function\n",
        "$Q_\\theta(s, a)$ for the evaluation policy $\\pi_\\phi(s)$.\n",
        "\n",
        "The FQE loss is:\n",
        "\n",
        "$$\n",
        "    L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_{t+1}, s_{t+1}) \\sim D}\n",
        "        \\left[ \\left( Q_\\theta(s_t, a_t) - r_{t+1}\n",
        "            - \\gamma Q_{\\theta'}(s_{t+1}, \\pi_\\phi(s_{t+1})) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "where $D$ is the logged dataset, $\\theta'$ is the target network\n",
        "parameters (soft-updated with $\\tau$), and $\\pi_\\phi(s_{t+1})$\n",
        "is the deterministic action from the evaluation policy.\n",
        "\n",
        "The trained Q function in FQE estimates evaluation metrics more accurately\n",
        "than the Q function learned during policy training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#QUESTION：好像没有做goal-conditioned？现在是把整个obs('observation', 'desired_goal', 'achieved_goal')拉平作为一个obs。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
              "       0.5      , 0.25     , 0.2082162, 0.4549032, 0.4888545, 0.2      ,\n",
              "       0.5      , 0.5      ], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['observation', 'desired_goal', 'achieved_goal'])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0.5      , 0.5145179, 0.5      , 0.2      , 0.5      , 0.5      ,\n",
              "       0.5      , 0.25     ], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0.2, 0.5, 0.5], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0.2082162, 0.4549032, 0.4888545], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(dataset.obs_flat[0])\n",
        "display(dataset.obs_dict[0].keys())\n",
        "display(dataset.obs_dict[0]['observation'])\n",
        "display(dataset.obs_dict[0]['achieved_goal'])\n",
        "display(dataset.obs_dict[0]['desired_goal'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0001 | FQE loss=378.711\n",
            "Epoch 0050 | FQE loss=343.292\n",
            "Epoch 0100 | FQE loss=314.811\n",
            "Epoch 0150 | FQE loss=306.488\n",
            "Epoch 0200 | FQE loss=301.351\n",
            "Epoch 0250 | FQE loss=268.231\n",
            "Epoch 0300 | FQE loss=236.435\n"
          ]
        }
      ],
      "source": [
        "# 训练 FQE\n",
        "#TODO: FQE train和predict的过程，放到build_ope_inputs里去，不对外暴露\n",
        "obs_dim = dataset.obs_flat.shape[1]\n",
        "act_dim = dataset.actions.shape[1]\n",
        "\n",
        "fqe = FQETrainer(\n",
        "    obs_dim=obs_dim,\n",
        "    act_dim=act_dim,\n",
        "    eval_algo=eval_algo,\n",
        "    gamma=gamma,\n",
        "    lr=3e-4,\n",
        "    tau=0.005,\n",
        ")\n",
        "\n",
        "loss_log = []\n",
        "\n",
        "\n",
        "def _logger(epoch: int, loss: float):\n",
        "    if epoch % 50 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:04d} | FQE loss={loss:.3f}\")\n",
        "    loss_log.append((epoch, loss))\n",
        "\n",
        "\n",
        "fqe.fit(dataset, batch_size=256, n_epochs=300, shuffle=True, logger=_logger)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 构造 OPE 输入\n",
        "Build OPE inputs from logged dataset and trained FQE model.\n",
        "\n",
        "Computes evaluation policy actions/log-probs and Q-values needed for\n",
        "DM, TIS, and DR estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构造 OPE 输入\n",
        "#TODO: FQE train和predict的过程，放到build_ope_inputs里去。实现起来，传入参数：q_function_method\n",
        "inputs = build_ope_inputs(dataset, eval_algo=eval_algo, fqe=fqe, gamma=gamma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['obs_flat', 'actions', 'rewards', 'next_obs_flat', 'dones', 'traj_id', 'step_index', 'behavior_log_prob', 'eval_action', 'eval_log_prob', 'q_sa_behavior', 'q_sa_eval', 'gamma'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.__dict__.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 计算 DM / TIS / DR\n",
        "### DM\n",
        "Direct Method (DM) estimator.\n",
        "\n",
        "DM estimates the policy value using the FQE Q-function:\n",
        "\n",
        "$$\n",
        "\n",
        "    \\hat{V}^{\\text{DM}} = \\frac{1}{N} \\sum_{i=1}^N Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
        "\n",
        "If ``initial_only=True``, only uses initial states (step_index == 0):\n",
        "\n",
        "$$\n",
        "\n",
        "    \\hat{V}^{\\text{DM}} = \\frac{1}{|\\mathcal{I}_0|} \\sum_{i \\in \\mathcal{I}_0} Q(s_i, \\pi_{\\text{eval}}(s_i)) $$\n",
        "\n",
        "where $\\mathcal{I}_0$ is the set of initial state indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TIS\n",
        "Trajectory-wise Importance Sampling (TIS) estimator.\n",
        "\n",
        "TIS estimates the policy value using trajectory-level importance weights:\n",
        "\n",
        "$$\n",
        "\n",
        "    \\hat{V}^{\\text{TIS}} = \\frac{1}{M} \\sum_{\\tau=1}^M w_\\tau G_\\tau\n",
        "\n",
        "$$\n",
        "\n",
        "where $M$ is the number of trajectories, $G_\\tau$ is the\n",
        "discounted return of trajectory $\\tau$, and the importance weight is:\n",
        "\n",
        "$$\n",
        "\n",
        "    w_\\tau = \\prod_{t=0}^{T_\\tau-1} \\frac{\\pi_{\\text{eval}}(a_t | s_t)}{\\pi_{\\text{behavior}}(a_t | s_t)}\n",
        "        = \\exp\\left( \\sum_{t=0}^{T_\\tau-1} \\left( \\log \\pi_{\\text{eval}}(a_t | s_t)\n",
        "            - \\log \\pi_{\\text{behavior}}(a_t | s_t) \\right) \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DR\n",
        "Doubly Robust (DR) estimator.\n",
        "\n",
        "DR combines importance sampling with a control variate (Q-function) to\n",
        "reduce variance. For each trajectory $\\tau$, the estimate is:\n",
        "\n",
        "$$\n",
        "\n",
        "    \\hat{V}_\\tau^{\\text{DR}} = \\sum_{t=0}^{T_\\tau-1} \\gamma^t \\left[\n",
        "        w_t (r_t - Q(s_t, a_t)) + w_{t-1} Q(s_t, \\pi_{\\text{eval}}(s_t))\n",
        "    \\right]\n",
        "$$\n",
        "\n",
        "where $w_t = \\prod_{k=0}^t \\frac{\\pi_{\\text{eval}}(a_k | s_k)}{\\pi_{\\text{behavior}}(a_k | s_k)}$\n",
        "is the step-wise importance weight, $w_{-1} = 1$, and\n",
        "$Q(s_t, a_t)$ is the Q-value for the behavior action while\n",
        "$Q(s_t, \\pi_{\\text{eval}}(s_t))$ is for the evaluation policy action.\n",
        "\n",
        "The overall estimate is:\n",
        "\n",
        "$$\n",
        "\n",
        "    \\hat{V}^{\\text{DR}} = \\frac{1}{M} \\sum_{\\tau=1}^M \\hat{V}_\\tau^{\\text{DR}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DM (step-wise): EstimateResult(mean=-27.3273868560791, ci_lower=-27.494298934936523, ci_upper=-27.148042678833008)\n",
            "TIS: EstimateResult(mean=-2.541983344700355e+21, ci_lower=-7.689500609917867e+21, ci_upper=-6.466416358947754)\n",
            "DR: EstimateResult(mean=-1.87473706078356e+21, ci_lower=-5.671088024972073e+21, ci_upper=5695156699267072.0)\n"
          ]
        }
      ],
      "source": [
        "#TODO: 需要修改，初始状态的Q值，需要用初始状态的Q值，而不是用FQE的Q值？\n",
        "#TODO: 将计算各轨迹的v值，和计算mean&ci的过程，分开。\n",
        "dm_all = dm_estimate(inputs, initial_only=False)\n",
        "# dm_init = dm_estimate(inputs, initial_only=True)\n",
        "tis_res = tis_estimate(inputs)\n",
        "dr_res = dr_estimate(inputs)\n",
        "\n",
        "print(\"DM (step-wise):\", dm_all)\n",
        "# print(\"DM (initial-state):\", dm_init)\n",
        "print(\"TIS:\", tis_res)\n",
        "print(\"DR:\", dr_res)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "print, Train, \u001b[31mnegative_overload_and_big_phi_termination。\u001b[0m steps: 251。target: (206.70, 8.42, -17.65)。achieved target: (70.86, -15.47, 25.64)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 58。target: (235.09, -6.62, 27.86)。achieved target: (178.99, 3.42, 52.36)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 93。target: (212.37, 2.14, 28.23)。achieved target: (162.29, 15.84, 84.77)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 84。target: (228.70, 5.80, -26.75)。achieved target: (168.67, 19.70, 76.02)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 35。target: (186.93, -8.30, -18.39)。achieved target: (192.96, 1.07, 20.23)。expert steps: 0。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maxine/ai4robot/gc_ope/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 122。target: (179.68, -0.14, 20.97)。achieved target: (186.12, -14.12, 25.64)。expert steps: 0。\n",
            "print, Train, \u001b[31mtimeout_termination。\u001b[0m steps: 399。target: (246.52, 4.16, -17.18)。achieved target: (265.60, -1.20, 35.98)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 54。target: (204.50, 4.12, -26.89)。achieved target: (193.27, 9.00, 14.09)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 101。target: (217.99, -2.63, 5.38)。achieved target: (208.04, -8.86, 10.96)。expert steps: 0。\n",
            "print, Train, \u001b[31mcontinuousely_move_away_termination_based_on_mu_error_and_chi_error。\u001b[0m steps: 59。target: (216.95, 3.38, 1.38)。achieved target: (199.45, -0.20, 9.97)。expert steps: 0。\n",
            "behavior return: -209.68 ± 31.57\n",
            "eval return:     -180.59 ± 3.18\n"
          ]
        }
      ],
      "source": [
        "# 在线评估真实回报（可选，耗时）\n",
        "if True:\n",
        "    mean_r_b, std_r_b = evaluate_policy(behavior_algo, env, n_eval_episodes=5, deterministic=True)\n",
        "    mean_r_e, std_r_e = evaluate_policy(eval_algo, env, n_eval_episodes=5, deterministic=True)\n",
        "    print(f\"behavior return: {mean_r_b:.2f} ± {std_r_b:.2f}\")\n",
        "    print(f\"eval return:     {mean_r_e:.2f} ± {std_r_e:.2f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
